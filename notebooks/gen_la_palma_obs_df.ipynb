{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries related to querying links and downloading files from the web\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "from pipmag import la_palma_utils as lp\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first entry: 2013-06-30\n",
      "last entry : 2023-05-16\n",
      "total observing dates: 122\n",
      "total number of media links: 8587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the years for which the La Palma Observatory has data at UiO\n",
    "obs_years = lp.get_obs_years()\n",
    "\n",
    "# Get the observing dates for all the years\n",
    "obs_dates = lp.get_obs_dates(obs_years)\n",
    "obs_dates_list = lp.get_obs_dates_list(obs_dates)\n",
    "\n",
    "# print the first, last and total number of observing dates\n",
    "print(f'first entry: {obs_dates_list[0]}\\n'\n",
    "      f'last entry : {obs_dates_list[-1]}\\n'\n",
    "      f'total observing dates: {len(obs_dates_list)}')\n",
    "\n",
    "# get the latest file from the list of files in the data directory\n",
    "media_links_file = '../data/all_media_links.csv'\n",
    "latest_all_media_links_file = media_links_file if os.path.isfile(media_links_file) else None\n",
    "\n",
    "# check if all_media_links.pkl exists then load the pickle file, otherwise get the links\n",
    "if latest_all_media_links_file is None:\n",
    "    video_links = lp.get_video_liks(obs_dates)  # get the video links, one for each observing date\n",
    "    image_links = lp.get_image_links(obs_dates)  # get the image links, one for each observing date\n",
    "    all_image_links = lp.get_all_links(image_links)  # get all the image links, one for each image\n",
    "    all_video_links = lp.get_all_links(video_links)  # get all the video links, one for each video\n",
    "    # print the number of video and image links and all the video and image links\n",
    "    print(f'number of video links: {len(all_video_links)}\\nnumber of image links: {len(all_image_links)}')\n",
    "    print(f'video links: {len(all_video_links)}\\nimage links: {len(all_image_links)}')\n",
    "    all_media_links = all_image_links + all_video_links  # combine the image and video links\n",
    "    all_media_links = sorted(all_media_links)  # sort the list of links\n",
    "    # print the total number of media links\n",
    "    print(f'total number of media links: {len(all_media_links)}')\n",
    "    # convert the all_media_links list to dataframe\n",
    "    links_df = pd.DataFrame(all_media_links, columns=['Links'])\n",
    "    # save dataframe to csv file\n",
    "    links_df.to_csv('../data/all_media_links.csv', index=False)\n",
    "    print('All media links have been saved as a CSV file.')\n",
    "else:\n",
    "    # load the media links csv file\n",
    "    links_df = pd.read_csv('../data/all_media_links.csv')\n",
    "    # convert dataframe to list\n",
    "    all_media_links = links_df['Links'].tolist()\n",
    "    print(f'total number of media links: {len(all_media_links)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of links with date and time: 8524\n",
      "number of links without date and time: 63\n",
      "All dates in date_time_list are valid\n",
      "number of unique date_time_from_all_media_links_datetime values: 858\n",
      "first entry: 2013-06-30 09:15:50\n",
      "last entry : 2023-05-16 17:25:03\n",
      "total entries: 8524\n",
      "first entry: 2013-06-30 09:15:50\n",
      "last entry : 2023-05-16 17:25:03\n",
      "total entries: 858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the date and time from the links and find the links that do not have date and time and save them as a list\n",
    "date_time_from_all_media_links, date_time_not_found = lp.get_date_time_from_link_list(all_media_links)\n",
    "# remove all the links that do not have a date and time from all_media_links\n",
    "all_media_links_with_date_time = [link for link in all_media_links if link not in date_time_not_found]\n",
    "# print the number of links that contain date and time and the number of links that do not contain date and time\n",
    "print(f'number of links with date and time: {len(all_media_links_with_date_time)}\\n'\n",
    "      f'number of links without date and time: {len(date_time_not_found)}')\n",
    "invalid_dates = lp.get_invalid_dates(date_time_from_all_media_links)\n",
    "# remove the entries from date_time_from_all_media_links that are not in the correct format\n",
    "date_time_from_all_media_links = [date for date in date_time_from_all_media_links if date not in invalid_dates]\n",
    "# find the string pattern before the underscore in the invalid dates\n",
    "# and search for the pattern in the links with date and time\n",
    "# and save the links that contain the pattern in a list\n",
    "invalid_dates_pattern = [re.search(r'(.+?)_', date).group(1) for date in invalid_dates]\n",
    "# find the links that contain the pattern in invalid_dates_pattern and save them in a list\n",
    "invalid_dates_links = [link for link in all_media_links_with_date_time\n",
    "                       if any(pattern in link for pattern in invalid_dates_pattern)]\n",
    "\n",
    "# convert the date and time to datetime format\n",
    "date_time_from_all_media_links_datetime = lp.convert_to_datetime(date_time_from_all_media_links)\n",
    "# get the unique date_time_from_all_media_links_datetime  values\n",
    "unique_date_time_from_all_media_links_datetime = list(set(date_time_from_all_media_links_datetime))\n",
    "# print the number of unique date_time_from_all_media_links_datetime values\n",
    "print(f'number of unique date_time_from_all_media_links_datetime values: '\n",
    "      f'{len(unique_date_time_from_all_media_links_datetime)}')\n",
    "# create a dataframe with the date_time_from_all_media_links_datetime as the index and the all_media_links as the column\n",
    "df = pd.DataFrame(all_media_links_with_date_time, index=date_time_from_all_media_links_datetime, columns=['links'])\n",
    "# print first, last and total number of entries in the dataframe\n",
    "print(f'first entry: {df.index[0]}\\nlast entry : {df.index[-1]}\\ntotal entries: {len(df.index)}')\n",
    "\n",
    "# group the dataframe by the time index and combine the links into a list\n",
    "df = df.groupby(df.index).agg({'links': lambda x: list(x)})\n",
    "# print the first, last and total number of entries in the dataframe\n",
    "print(f'first entry: {df.index[0]}\\nlast entry : {df.index[-1]}\\ntotal entries: {len(df.index)}')\n",
    "\n",
    "# add a column called 'obs_id' and set it equal to the row number of the dataframe\n",
    "# add the 'id' column\n",
    "df['obs_id'] = range(0, len(df))\n",
    "# set the index as 'obs_id' and add a column for the date and time\n",
    "df['date_time'] = df.index\n",
    "df = df.set_index('obs_id')\n",
    "# add a column for the number of links in each row\n",
    "df['num_links'] = df['links'].apply(lambda x: len(x))\n",
    "# add columns for the year, month and day to the dataframe\n",
    "df['year'] = df['date_time'].apply(lambda x: x.year)\n",
    "df['month'] = df['date_time'].apply(lambda x: x.month)\n",
    "df['day'] = df['date_time'].apply(lambda x: x.day)\n",
    "# add a column for the time of day\n",
    "df['time'] = df['date_time'].apply(lambda x: x.time())\n",
    "# add a column called 'target' and set it equal to None\n",
    "df['target'] = None\n",
    "df['comments'] = None\n",
    "df['polarimetry'] = None\n",
    "instrument_keywords = {\n",
    "    'CRISP': ['wb_6563', 'ha', 'Crisp', '6173', '8542', '6563', 'crisp'],\n",
    "    'CHROMIS': ['Chromis', 'cak', '4846'],\n",
    "    'IRIS': ['sji']\n",
    "}\n",
    "# apply the get_instrument_info function to the 'links' column of the dataframe\n",
    "# and add the result to a new column called 'instruments'\n",
    "df['instruments'] = df['links'].apply(lambda x: lp.get_instrument_info(x, instrument_keywords))\n",
    "# apply the get_links_with_string function to the 'links' column of the dataframe with the strings 'mp4' and 'mov'\n",
    "# and add the result to a new column called 'video_links'\n",
    "df['video_links'] = df['links'].apply(lambda x: lp.get_links_with_string(x, ['mp4', 'mov']))\n",
    "# apply the get_links_with_string function to the 'links' column of the dataframe with the strings 'jpg' and 'png'\n",
    "# and add the result to a new column called 'image_links'\n",
    "df['image_links'] = df['links'].apply(lambda x: lp.get_links_with_string(x, ['jpg', 'png']))\n",
    "# pm.get_links_with_string(df.iloc[0]['links'], ['mp4','mov'])\n",
    "# make the columns date-time, year, month, day, time, instruments, target, video_links, image_links, links, num_links\n",
    "df = df[['date_time', 'year', 'month', 'day', 'time', 'instruments', 'target',\n",
    "         'comments', 'video_links', 'image_links', 'links', 'num_links', 'polarimetry']]\n",
    "\n",
    "# === Fix duplicate times === #\n",
    "\n",
    "# Convert 'date_time' column to DateTime type\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "# Sort the DataFrame by 'date_time'\n",
    "sorted_df = df.sort_values('date_time')\n",
    "\n",
    "# Define threshold for time difference\n",
    "threshold = timedelta(seconds=60)\n",
    "\n",
    "# Group rows based on time proximity\n",
    "grouped_df = sorted_df.groupby((sorted_df['date_time'].diff() > threshold).cumsum()).agg({\n",
    "    'date_time': 'first',\n",
    "    'year': 'first',\n",
    "    'month': 'first',\n",
    "    'day': 'first',\n",
    "    'time': 'first',\n",
    "    'instruments': 'sum',\n",
    "    'target': 'first',\n",
    "    'comments': 'first',\n",
    "    'video_links': 'sum',\n",
    "    'image_links': 'sum',\n",
    "    'links': 'sum',\n",
    "    'num_links': 'sum',\n",
    "    'polarimetry': 'min'\n",
    "})\n",
    "\n",
    "# convert the 'date_time' column back\n",
    "grouped_df['date_time'] = grouped_df['date_time'].apply(lambda x: x.to_pydatetime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 666 entries, 0 to 665\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   date_time    666 non-null    datetime64[ns]\n",
      " 1   year         666 non-null    int64         \n",
      " 2   month        666 non-null    int64         \n",
      " 3   day          666 non-null    int64         \n",
      " 4   time         666 non-null    object        \n",
      " 5   instruments  666 non-null    object        \n",
      " 6   target       0 non-null      object        \n",
      " 7   comments     0 non-null      object        \n",
      " 8   video_links  666 non-null    object        \n",
      " 9   image_links  666 non-null    object        \n",
      " 10  links        666 non-null    object        \n",
      " 11  num_links    666 non-null    int64         \n",
      " 12  polarimetry  0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(7)\n",
      "memory usage: 72.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Convert the lists in 'links', 'video_links', 'image_links', 'instruments' columns to strings\n",
    "grouped_df['links'] = grouped_df['links'].apply(lambda x: ','.join(x))\n",
    "grouped_df['video_links'] = grouped_df['video_links'].apply(lambda x: ','.join(x))\n",
    "grouped_df['image_links'] = grouped_df['image_links'].apply(lambda x: ','.join(x))\n",
    "grouped_df['instruments'] = grouped_df['instruments'].apply(lambda x: ','.join(x))\n",
    "# print a summary of the dataframe\n",
    "grouped_df.info()\n",
    "# Save the DataFrame to a CSV file\n",
    "grouped_df.to_csv('../data/la_palma_obs_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipmag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
