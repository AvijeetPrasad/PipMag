{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries related to querying links and downloading files from the web\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "from IPython.display import display, clear_output, Video\n",
    "import importlib\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from pipmag import pipmag as pm\n",
    "# If you get the error ModuleNotFoundError: No module named 'pipmag', run the following line and restart the kernel:\n",
    "# %pip install -e ..\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported updated pipmag.py\n"
     ]
    }
   ],
   "source": [
    "# reload the pipmag module to make sure that the latest version is used\n",
    "importlib.reload(pm)\n",
    "print('Imported updated pipmag.py')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Generating the dataframe from SST quicklooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the years for which the La Palma Observatory has data at UiO\n",
    "obs_years = pm.get_obs_years()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first entry: 2013-06-30\n",
      "last entry : 2023-05-16\n",
      "total observing dates: 122\n"
     ]
    }
   ],
   "source": [
    "# Get the observing dates for all the years\n",
    "obs_dates = pm.get_obs_dates(obs_years)\n",
    "obs_dates_list = pm.get_obs_dates_list(obs_dates)\n",
    "# print the first, last and total number of observing dates\n",
    "print(f'first entry: {obs_dates_list[0]}\\nlast entry : {obs_dates_list[-1]}\\ntotal observing dates: {len(obs_dates_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file: ../data/all_media_links_20230706_081644.pkl\n"
     ]
    }
   ],
   "source": [
    "# get the latest file from the list of files in the data directory\n",
    "latest_all_media_links_file = pm.get_latest_file('../data/all_media_links*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ../data/all_media_links_20230706_081644.pkl successfully\n",
      "total number of media links: 8587\n"
     ]
    }
   ],
   "source": [
    "# check if all_media_links.pkl exists then load the pickle file, otherwise get the links\n",
    "if latest_all_media_links_file is None:\n",
    "    video_links = pm.get_video_liks(obs_dates) # get the video links, one for each observing date\n",
    "    image_links = pm.get_image_links(obs_dates) # get the image links, one for each observing date\n",
    "    all_image_links = pm.get_all_links(image_links) # get all the image links, one for each image\n",
    "    all_video_links = pm.get_all_links(video_links) # get all the video links, one for each video\n",
    "    # print the number of video and image links and all the video and image links \n",
    "    print(f'number of video links: {len(all_video_links)}\\nnumber of image links: {len(all_image_links)}')\n",
    "    print(f'video links: {len(all_video_links)}\\nimage links: {len(all_image_links)}')\n",
    "    all_media_links = all_image_links + all_video_links # combine the image and video links\n",
    "    all_media_links = sorted(all_media_links) #sort the list of links\n",
    "    # print the total number of media links\n",
    "    print(f'total number of media links: {len(all_media_links)}')\n",
    "    # save all the media links as a pickle file\n",
    "    pm.save_pickle(all_media_links, '../data/'+ pm.add_timestamp('all_media_links.pkl'))\n",
    "else:\n",
    "    # load the latest pickle file\n",
    "    all_media_links = pm.load_pickle(latest_all_media_links_file )\n",
    "    print(f'total number of media links: {len(all_media_links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of links with date and time: 8524\n",
      "number of links without date and time: 63\n",
      "All dates in date_time_list are valid\n"
     ]
    }
   ],
   "source": [
    "# get the date and time from the links and find the links that do not have date and time and save them as a list\n",
    "date_time_from_all_media_links, date_time_not_found = pm.get_date_time_from_link_list(all_media_links)\n",
    "# remove all the links that do not have a date and time from all_media_links\n",
    "all_media_links_with_date_time = [link for link in all_media_links if link not in date_time_not_found]\n",
    "# print the number of links that contain date and time and the number of links that do not contain date and time\n",
    "print(f'number of links with date and time: {len(all_media_links_with_date_time)}\\nnumber of links without date and time: {len(date_time_not_found)}')\n",
    "invalid_dates = pm.get_invalid_dates(date_time_from_all_media_links)\n",
    "# remove the entries from date_time_from_all_media_links that are not in the correct format\n",
    "date_time_from_all_media_links = [date for date in date_time_from_all_media_links if date not in invalid_dates]\n",
    "# find the string pattern before the underscore in the invalid dates and search for the pattern in the links with date and time and save the links that contain the pattern in a list\n",
    "invalid_dates_pattern = [re.search(r'(.+?)_', date).group(1) for date in invalid_dates]\n",
    "# find the links that contain the pattern in invalid_dates_pattern and save them in a list\n",
    "invalid_dates_links = [link for link in all_media_links_with_date_time if any(pattern in link for pattern in invalid_dates_pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique date_time_from_all_media_links_datetime values: 858\n",
      "first entry: 2013-06-30 09:15:50\n",
      "last entry : 2023-05-16 17:25:03\n",
      "total entries: 8524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert the date and time to datetime format\n",
    "date_time_from_all_media_links_datetime = pm.convert_to_datetime(date_time_from_all_media_links)\n",
    "# get the unique date_time_from_all_media_links_datetime  values\n",
    "unique_date_time_from_all_media_links_datetime = list(set(date_time_from_all_media_links_datetime))\n",
    "# print the number of unique date_time_from_all_media_links_datetime values\n",
    "print(f'number of unique date_time_from_all_media_links_datetime values: {len(unique_date_time_from_all_media_links_datetime)}')\n",
    "# create a dataframe with the date_time_from_all_media_links_datetime as the index and the all_media_links as the column\n",
    "df = pd.DataFrame(all_media_links_with_date_time, index=date_time_from_all_media_links_datetime, columns=['links'])\n",
    "#print first, last and total number of entries in the dataframe\n",
    "print(f'first entry: {df.index[0]}\\nlast entry : {df.index[-1]}\\ntotal entries: {len(df.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first entry: 2013-06-30 09:15:50\n",
      "last entry : 2023-05-16 17:25:03\n",
      "total entries: 858\n"
     ]
    }
   ],
   "source": [
    "# group the dataframe by the time index and combine the links into a list\n",
    "df = df.groupby(df.index).agg({'links': lambda x: list(x)})\n",
    "# print the first, last and total number of entries in the dataframe\n",
    "print(f'first entry: {df.index[0]}\\nlast entry : {df.index[-1]}\\ntotal entries: {len(df.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column called 'obs_id' and set it equal to the row number of the dataframe\n",
    "# add the 'id' column\n",
    "df['obs_id'] = range(0, len(df))\n",
    "# set the index as 'obs_id' and add a column for the date and time\n",
    "df['date_time'] = df.index\n",
    "df = df.set_index('obs_id')\n",
    "# add a column for the number of links in each row\n",
    "df['num_links'] = df['links'].apply(lambda x: len(x))\n",
    "# add columns for the year, month and day to the dataframe\n",
    "df['year'] = df['date_time'].apply(lambda x: x.year)\n",
    "df['month'] = df['date_time'].apply(lambda x: x.month)\n",
    "df['day'] = df['date_time'].apply(lambda x: x.day)\n",
    "# add a column for the time of day\n",
    "df['time'] = df['date_time'].apply(lambda x: x.time())\n",
    "# add a column called 'target' and set it equal to None\n",
    "df['target'] = None\n",
    "df['comments'] = None\n",
    "df['polarimetry'] = None\n",
    "instrument_keywords={'CRISP': ['wb_6563','ha','Crisp','6173','8542','6563','crisp'],'CHROMIS':['Chromis','cak','4846'],'IRIS':['sji']}\n",
    "# apply the get_instrument_info function to the 'links' column of the dataframe and add the result to a new column called 'instruments'\n",
    "df['instruments'] = df['links'].apply(lambda x: pm.get_instrument_info(x, instrument_keywords))\n",
    "# apply the get_links_with_string function to the 'links' column of the dataframe with the strings 'mp4' and 'mov' and add the result to a new column called 'video_links'\n",
    "df['video_links'] = df['links'].apply(lambda x: pm.get_links_with_string(x, ['mp4','mov']))\n",
    "# apply the get_links_with_string function to the 'links' column of the dataframe with the strings 'jpg' and 'png' and add the result to a new column called 'image_links'\n",
    "df['image_links'] = df['links'].apply(lambda x: pm.get_links_with_string(x, ['jpg','png']))\n",
    "#pm.get_links_with_string(df.iloc[0]['links'], ['mp4','mov'])\n",
    "# make the columns date-time, year, month, day, time, instruments, target, video_links, image_links, links, num_links\n",
    "df = df[['date_time', 'year', 'month', 'day', 'time', 'instruments', 'target', 'comments','video_links', 'image_links', 'links', 'num_links','polarimetry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 666 entries, 0 to 665\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   date_time    666 non-null    datetime64[ns]\n",
      " 1   year         666 non-null    int64         \n",
      " 2   month        666 non-null    int64         \n",
      " 3   day          666 non-null    int64         \n",
      " 4   time         666 non-null    object        \n",
      " 5   instruments  666 non-null    object        \n",
      " 6   target       0 non-null      object        \n",
      " 7   comments     0 non-null      object        \n",
      " 8   video_links  666 non-null    object        \n",
      " 9   image_links  666 non-null    object        \n",
      " 10  links        666 non-null    object        \n",
      " 11  num_links    666 non-null    int64         \n",
      " 12  polarimetry  0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(7)\n",
      "memory usage: 72.8+ KB\n"
     ]
    }
   ],
   "source": [
    "## Fix duplicate times ## \n",
    "\n",
    "# Convert 'date_time' column to DateTime type\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "# Sort the DataFrame by 'date_time'\n",
    "df_ = df.sort_values('date_time')\n",
    "\n",
    "# Define threshold for time difference \n",
    "threshold = timedelta(seconds=60) \n",
    "\n",
    "# Group rows based on time proximity\n",
    "grouped_df = df_.groupby((df_['date_time'].diff() > threshold).cumsum()).agg({\\\n",
    "    'date_time': 'first', 'year': 'first', 'month': 'first', 'day': 'first', 'time': 'first', 'instruments': 'sum', \\\n",
    "        'target': 'first', 'comments': 'first', 'video_links': 'sum', 'image_links': 'sum', 'links': 'sum', \\\n",
    "            'num_links': 'sum', 'polarimetry': 'min'})\n",
    "\n",
    "# convert the 'date_time' column back \n",
    "grouped_df['date_time'] = grouped_df['date_time'].apply(lambda x: x.to_pydatetime())\n",
    "\n",
    "# print a summary of the dataframe\n",
    "grouped_df.info()\n",
    "# save the dataframe as a pickle file\n",
    "grouped_df.to_pickle('../data/'+ pm.add_timestamp('la_palma_obs_data.pkl'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➡️ Start here : Load the existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file: ../data/la_palma_obs_data_20230706_081731.pkl\n"
     ]
    }
   ],
   "source": [
    "# get the latest pickle file\n",
    "latest_updated_la_palma_obs_data_file = pm.get_latest_file('../data/la_palma_obs_data_*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 666 entries, 0 to 665\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   date_time    666 non-null    datetime64[ns]\n",
      " 1   year         666 non-null    int64         \n",
      " 2   month        666 non-null    int64         \n",
      " 3   day          666 non-null    int64         \n",
      " 4   time         666 non-null    object        \n",
      " 5   instruments  666 non-null    object        \n",
      " 6   target       0 non-null      object        \n",
      " 7   comments     0 non-null      object        \n",
      " 8   video_links  666 non-null    object        \n",
      " 9   image_links  666 non-null    object        \n",
      " 10  links        666 non-null    object        \n",
      " 11  num_links    666 non-null    int64         \n",
      " 12  polarimetry  0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(7)\n",
      "memory usage: 72.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(latest_updated_la_palma_obs_data_file)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226d839466d7463b9f900e013488e25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Year:', options=(2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023), valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899a7f1303794ce9a3e038c529aacf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Month:', options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1b8e7d896f48ae9ac1889cc453dbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Day:', options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bf40b6e5c34b7bb9a6efdd4053609e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Time:', options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdc5a69380843868a55889f53b016cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Links:', options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e849e3847ef4fc19c2e5c936bb5759f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0424c4c35354e1bb2994c1a10d713c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd70acdb5ebe42de9fd053eb70678cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='target:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec6e90a68b34799a37116e97800cabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='instruments:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952d896c26f041c98a7f9b7f486b23e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='polarimetry:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f3d54e6d9747db836641c59f8e5de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='comments:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdde6a85b054670952dd17d9bac2bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Update', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a widget to display movies based on year, month, day and time\n",
    "# and to update the target, instrumnets and comments columns of the dataframe\n",
    "selector = pm.VideoSelector2(df, ['target', 'instruments', 'polarimetry', 'comments'])\n",
    "selector.create_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 ADS Search\n",
    "index = 36\n",
    "search = pm.ADS_Search(df)\n",
    "search.get_results(index, pretty_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated dataframe as a pickle file\n",
    "df.to_pickle('data/'+ pm.add_timestamp('la_palma_obs_data.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipmag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba2b68964390c77ef2e04277debd5475c8565a11fab30e531aaf9c5e1dfcc05e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
